{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# YOLOv5s Training & Export for Android Real-time Detection\n",
        "\n",
        "This notebook sets up, trains, and exports a YOLOv5s model for Android real-time detection applications.\n",
        "\n",
        "## Requirements\n",
        "- Python 3.12\n",
        "- CUDA-compatible GPU (recommended)\n",
        "- Dataset in YOLO format\n",
        "\n",
        "## Model Configuration\n",
        "- **Model**: YOLOv5s (small variant)\n",
        "- **Image Size**: 640x640\n",
        "- **Batch Size**: 16\n",
        "- **Epochs**: 100\n",
        "- **Classes**: nsfw, gore"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Test Model Inference (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import glob\n",
        "\n",
        "# Test the trained model on a sample image\n",
        "def test_model_inference():\n",
        "    # Find trained model\n",
        "    weights_pattern = \"Thesis/models/yolov5s_training*/weights/best.pt\"\n",
        "    weight_files = glob.glob(weights_pattern)\n",
        "    \n",
        "    if not weight_files:\n",
        "        print(\"‚ùå No trained model found\")\n",
        "        return\n",
        "    \n",
        "    best_weights = weight_files[0]\n",
        "    print(f\"Testing model: {best_weights}\")\n",
        "    \n",
        "    # Find a test image\n",
        "    test_images = list(Path(\"Thesis/dataset/images/val\").glob(\"*.jpg\")) + \\\n",
        "                 list(Path(\"Thesis/dataset/images/val\").glob(\"*.png\"))\n",
        "    \n",
        "    if not test_images:\n",
        "        print(\"‚ùå No test images found in validation set\")\n",
        "        return\n",
        "    \n",
        "    test_image = test_images[0]\n",
        "    print(f\"Testing on image: {test_image}\")\n",
        "    \n",
        "    # Load model\n",
        "    os.chdir('yolov5')\n",
        "    try:\n",
        "        model = torch.hub.load('.', 'custom', path=f\"../{best_weights}\", source='local')\n",
        "        model.conf = 0.25  # confidence threshold\n",
        "        model.iou = 0.45   # IoU threshold\n",
        "        \n",
        "        # Run inference\n",
        "        results = model(f\"../{test_image}\")\n",
        "        \n",
        "        # Display results\n",
        "        results.show()  # This will save results with bounding boxes\n",
        "        \n",
        "        # Print detection results\n",
        "        detections = results.pandas().xyxy[0]\n",
        "        if len(detections) > 0:\n",
        "            print(f\"\\nüéØ Detections found: {len(detections)}\")\n",
        "            for idx, detection in detections.iterrows():\n",
        "                print(f\"  Class: {detection['name']}, Confidence: {detection['confidence']:.3f}\")\n",
        "        else:\n",
        "            print(\"\\nüîç No detections found\")\n",
        "        \n",
        "        # Save results to output folder\n",
        "        results.save(save_dir='../Thesis/output/test_results')\n",
        "        print(f\"\\nüíæ Test results saved to: Thesis/output/test_results/\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Inference error: {str(e)}\")\n",
        "    \n",
        "    os.chdir('..')\n",
        "\n",
        "# Run the test\n",
        "test_model_inference()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Create Android Integration Guide"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Android integration guide\n",
        "android_guide = \"\"\"\n",
        "# Android Integration Guide for YOLOv5s TensorFlow Lite Model\n",
        "\n",
        "## Model Specifications\n",
        "- **Input Shape**: [1, 640, 640, 3]\n",
        "- **Input Type**: Float32\n",
        "- **Input Range**: [0, 1] (normalized)\n",
        "- **Output Shape**: [1, 25200, 7] for 2 classes (nsfw, gore)\n",
        "- **Output Format**: [x, y, w, h, confidence, class_0_prob, class_1_prob]\n",
        "\n",
        "## Android Dependencies\n",
        "Add to your app's build.gradle:\n",
        "```gradle\n",
        "implementation 'org.tensorflow:tensorflow-lite:2.13.0'\n",
        "implementation 'org.tensorflow:tensorflow-lite-support:0.4.4'\n",
        "implementation 'org.tensorflow:tensorflow-lite-gpu:2.13.0' // Optional for GPU acceleration\n",
        "```\n",
        "\n",
        "## Model Loading (Kotlin)\n",
        "```kotlin\n",
        "private lateinit var interpreter: Interpreter\n",
        "\n",
        "private fun loadModel() {\n",
        "    try {\n",
        "        val model = loadModelFile(\"yolov5s_best.tflite\")\n",
        "        val options = Interpreter.Options()\n",
        "        interpreter = Interpreter(model, options)\n",
        "    } catch (e: IOException) {\n",
        "        Log.e(\"TFLite\", \"Error loading model\", e)\n",
        "    }\n",
        "}\n",
        "\n",
        "private fun loadModelFile(filename: String): ByteBuffer {\n",
        "    val assetFileDescriptor = assets.openFd(filename)\n",
        "    val fileInputStream = FileInputStream(assetFileDescriptor.fileDescriptor)\n",
        "    val fileChannel = fileInputStream.channel\n",
        "    val startOffset = assetFileDescriptor.startOffset\n",
        "    val declaredLength = assetFileDescriptor.declaredLength\n",
        "    return fileChannel.map(FileChannel.MapMode.READ_ONLY, startOffset, declaredLength)\n",
        "}\n",
        "```\n",
        "\n",
        "## Image Preprocessing\n",
        "```kotlin\n",
        "private fun preprocessImage(bitmap: Bitmap): FloatArray {\n",
        "    val resized = Bitmap.createScaledBitmap(bitmap, 640, 640, true)\n",
        "    val input = FloatArray(640 * 640 * 3)\n",
        "    \n",
        "    val pixels = IntArray(640 * 640)\n",
        "    resized.getPixels(pixels, 0, 640, 0, 0, 640, 640)\n",
        "    \n",
        "    for (i in pixels.indices) {\n",
        "        val pixel = pixels[i]\n",
        "        input[i * 3] = ((pixel shr 16) and 0xFF) / 255.0f     // R\n",
        "        input[i * 3 + 1] = ((pixel shr 8) and 0xFF) / 255.0f  // G\n",
        "        input[i * 3 + 2] = (pixel and 0xFF) / 255.0f          // B\n",
        "    }\n",
        "    \n",
        "    return input\n",
        "}\n",
        "```\n",
        "\n",
        "## Running Inference\n",
        "```kotlin\n",
        "private fun runInference(inputArray: FloatArray): Array<FloatArray> {\n",
        "    val input = Array(1) { Array(640) { Array(640) { FloatArray(3) } } }\n",
        "    val output = Array(1) { Array(25200) { FloatArray(7) } }\n",
        "    \n",
        "    // Reshape input\n",
        "    for (i in 0 until 640) {\n",
        "        for (j in 0 until 640) {\n",
        "            for (k in 0 until 3) {\n",
        "                input[0][i][j][k] = inputArray[(i * 640 + j) * 3 + k]\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    interpreter.run(input, output)\n",
        "    return output\n",
        "}\n",
        "```\n",
        "\n",
        "## Post-processing with NMS\n",
        "```kotlin\n",
        "data class Detection(\n",
        "    val bbox: RectF,\n",
        "    val confidence: Float,\n",
        "    val classId: Int,\n",
        "    val className: String\n",
        ")\n",
        "\n",
        "private fun postprocess(output: Array<FloatArray>, confidenceThreshold: Float = 0.5f): List<Detection> {\n",
        "    val detections = mutableListOf<Detection>()\n",
        "    val classNames = arrayOf(\"nsfw\", \"gore\")\n",
        "    \n",
        "    for (i in output[0].indices) {\n",
        "        val detection = output[0][i]\n",
        "        val confidence = detection[4]\n",
        "        \n",
        "        if (confidence > confidenceThreshold) {\n",
        "            val x = detection[0]\n",
        "            val y = detection[1]\n",
        "            val w = detection[2]\n",
        "            val h = detection[3]\n",
        "            \n",
        "            val classProbs = detection.sliceArray(5..6)\n",
        "            val classId = classProbs.indices.maxByOrNull { classProbs[it] } ?: 0\n",
        "            val classConfidence = classProbs[classId]\n",
        "            \n",
        "            val finalConfidence = confidence * classConfidence\n",
        "            \n",
        "            if (finalConfidence > confidenceThreshold) {\n",
        "                val bbox = RectF(\n",
        "                    x - w/2, y - h/2,\n",
        "                    x + w/2, y + h/2\n",
        "                )\n",
        "                \n",
        "                detections.add(\n",
        "                    Detection(bbox, finalConfidence, classId, classNames[classId])\n",
        "                )\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    return applyNMS(detections, 0.4f)\n",
        "}\n",
        "\n",
        "private fun applyNMS(detections: List<Detection>, iouThreshold: Float): List<Detection> {\n",
        "    val sortedDetections = detections.sortedByDescending { it.confidence }\n",
        "    val result = mutableListOf<Detection>()\n",
        "    \n",
        "    for (detection in sortedDetections) {\n",
        "        var shouldKeep = true\n",
        "        \n",
        "        for (kept in result) {\n",
        "            if (calculateIoU(detection.bbox, kept.bbox) > iouThreshold) {\n",
        "                shouldKeep = false\n",
        "                break\n",
        "            }\n",
        "        }\n",
        "        \n",
        "        if (shouldKeep) {\n",
        "            result.add(detection)\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    return result\n",
        "}\n",
        "\n",
        "private fun calculateIoU(box1: RectF, box2: RectF): Float {\n",
        "    val intersection = RectF()\n",
        "    intersection.setIntersect(box1, box2)\n",
        "    \n",
        "    val intersectionArea = intersection.width() * intersection.height()\n",
        "    val box1Area = box1.width() * box1.height()\n",
        "    val box2Area = box2.width() * box2.height()\n",
        "    val unionArea = box1Area + box2Area - intersectionArea\n",
        "    \n",
        "    return if (unionArea > 0) intersectionArea / unionArea else 0f\n",
        "}\n",
        "```\n",
        "\n",
        "## Real-time Processing\n",
        "```kotlin\n",
        "private fun processFrame(bitmap: Bitmap): Bitmap {\n",
        "    val inputArray = preprocessImage(bitmap)\n",
        "    val output = runInference(inputArray)\n",
        "    val detections = postprocess(output)\n",
        "    \n",
        "    return blurDetectedRegions(bitmap, detections)\n",
        "}\n",
        "\n",
        "private fun blurDetectedRegions(bitmap: Bitmap, detections: List<Detection>): Bitmap {\n",
        "    val canvas = Canvas(bitmap.copy(Bitmap.Config.ARGB_8888, true))\n",
        "    val paint = Paint()\n",
        "    \n",
        "    for (detection in detections) {\n",
        "        // Apply blur effect to detected region\n",
        "        val blurredRegion = createBlurredRegion(bitmap, detection.bbox)\n",
        "        canvas.drawBitmap(blurredRegion, detection.bbox.left, detection.bbox.top, paint)\n",
        "    }\n",
        "    \n",
        "    return bitmap\n",
        "}\n",
        "```\n",
        "\n",
        "## Performance Tips\n",
        "1. Use GPU delegation for faster inference:\n",
        "   ```kotlin\n",
        "   val options = Interpreter.Options()\n",
        "   options.addDelegate(GpuDelegate())\n",
        "   ```\n",
        "\n",
        "2. Use NNAPI delegation for optimized hardware acceleration:\n",
        "   ```kotlin\n",
        "   options.addDelegate(NnApiDelegate())\n",
        "   ```\n",
        "\n",
        "3. Process frames on background thread\n",
        "4. Consider reducing input resolution for real-time performance\n",
        "5. Implement frame skipping if processing is too slow\n",
        "\"\"\"\n",
        "\n",
        "# Save Android integration guide\n",
        "guide_path = \"Thesis/Android_Integration_Guide.md\"\n",
        "with open(guide_path, 'w') as f:\n",
        "    f.write(android_guide)\n",
        "\n",
        "print(f\"üì± Android integration guide saved to: {guide_path}\")\n",
        "print(\"\\nThe guide includes:\")\n",
        "print(\"- Model loading and initialization\")\n",
        "print(\"- Image preprocessing\")\n",
        "print(\"- Inference execution\")\n",
        "print(\"- Post-processing with NMS\")\n",
        "print(\"- Real-time processing example\")\n",
        "print(\"- Performance optimization tips\")"
      ],
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import subprocess\n",
        "\n",
        "# Check Python version\n",
        "print(f\"Python version: {sys.version}\")\n",
        "\n",
        "# Install required packages\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install ultralytics\n",
        "!pip install opencv-python\n",
        "!pip install matplotlib\n",
        "!pip install seaborn\n",
        "!pip install pandas\n",
        "!pip install pillow\n",
        "!pip install pyyaml\n",
        "!pip install tensorboard\n",
        "!pip install onnx\n",
        "!pip install tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Setup Folder Structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "# Create main Thesis directory and subdirectories\n",
        "base_dir = Path(\"Thesis\")\n",
        "folders = {\n",
        "    \"dataset\": [\"images/train\", \"images/val\", \"labels/train\", \"labels/val\"],\n",
        "    \"input\": [],\n",
        "    \"output\": [],\n",
        "    \"metrics\": [],\n",
        "    \"models\": []\n",
        "}\n",
        "\n",
        "# Create folder structure\n",
        "for main_folder, subfolders in folders.items():\n",
        "    main_path = base_dir / main_folder\n",
        "    main_path.mkdir(parents=True, exist_ok=True)\n",
        "    print(f\"Created: {main_path}\")\n",
        "    \n",
        "    for subfolder in subfolders:\n",
        "        sub_path = main_path / subfolder\n",
        "        sub_path.mkdir(parents=True, exist_ok=True)\n",
        "        print(f\"  Created: {sub_path}\")\n",
        "\n",
        "print(\"\\nFolder structure created successfully!\")\n",
        "print(\"\\nPlease ensure you have:\")\n",
        "print(\"- Training images in: Thesis/dataset/images/train/\")\n",
        "print(\"- Validation images in: Thesis/dataset/images/val/\")\n",
        "print(\"- Training labels in: Thesis/dataset/labels/train/\")\n",
        "print(\"- Validation labels in: Thesis/dataset/labels/val/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Create Dataset Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import yaml\n",
        "\n",
        "# Create data.yaml configuration file\n",
        "data_config = {\n",
        "    'train': 'dataset/images/train',\n",
        "    'val': 'dataset/images/val',\n",
        "    'nc': 2,  # number of classes\n",
        "    'names': ['nsfw', 'gore']\n",
        "}\n",
        "\n",
        "# Save data.yaml in dataset folder\n",
        "data_yaml_path = base_dir / \"dataset\" / \"data.yaml\"\n",
        "with open(data_yaml_path, 'w') as f:\n",
        "    yaml.dump(data_config, f, default_flow_style=False)\n",
        "\n",
        "print(f\"Dataset configuration saved to: {data_yaml_path}\")\n",
        "print(\"\\nDataset configuration:\")\n",
        "with open(data_yaml_path, 'r') as f:\n",
        "    print(f.read())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Clone and Setup YOLOv5 Repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import subprocess\n",
        "\n",
        "# Clone YOLOv5 repository if it doesn't exist\n",
        "yolo_dir = \"yolov5\"\n",
        "if not os.path.exists(yolo_dir):\n",
        "    !git clone https://github.com/ultralytics/yolov5.git\n",
        "    print(\"YOLOv5 repository cloned successfully!\")\n",
        "else:\n",
        "    print(\"YOLOv5 repository already exists.\")\n",
        "\n",
        "# Change to YOLOv5 directory and install requirements\n",
        "os.chdir(yolo_dir)\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "print(\"\\nYOLOv5 setup completed!\")\n",
        "print(f\"Current directory: {os.getcwd()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Verify Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Go back to main directory\n",
        "os.chdir('..')\n",
        "\n",
        "# Check dataset structure and count files\n",
        "dataset_base = Path(\"Thesis/dataset\")\n",
        "\n",
        "def count_files(directory, extension):\n",
        "    return len(list(directory.glob(f\"*.{extension}\")))\n",
        "\n",
        "# Count training and validation files\n",
        "train_images = count_files(dataset_base / \"images/train\", \"jpg\") + count_files(dataset_base / \"images/train\", \"png\")\n",
        "val_images = count_files(dataset_base / \"images/val\", \"jpg\") + count_files(dataset_base / \"images/val\", \"png\")\n",
        "train_labels = count_files(dataset_base / \"labels/train\", \"txt\")\n",
        "val_labels = count_files(dataset_base / \"labels/val\", \"txt\")\n",
        "\n",
        "print(\"Dataset Summary:\")\n",
        "print(f\"Training images: {train_images}\")\n",
        "print(f\"Training labels: {train_labels}\")\n",
        "print(f\"Validation images: {val_images}\")\n",
        "print(f\"Validation labels: {val_labels}\")\n",
        "\n",
        "if train_images == 0 or val_images == 0:\n",
        "    print(\"\\n‚ö†Ô∏è  WARNING: No images found! Please add your dataset before proceeding.\")\n",
        "    print(\"Expected format:\")\n",
        "    print(\"- Images: .jpg or .png files\")\n",
        "    print(\"- Labels: .txt files with YOLO format (class_id x_center y_center width height)\")\n",
        "else:\n",
        "    print(\"\\n‚úÖ Dataset verification completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Train YOLOv5s Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# Check GPU availability\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")\n",
        "if device == 'cuda':\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
        "\n",
        "# Training configuration\n",
        "img_size = 640\n",
        "batch_size = 16\n",
        "epochs = 100\n",
        "data_path = \"../Thesis/dataset/data.yaml\"\n",
        "project_name = \"../Thesis/models\"\n",
        "experiment_name = f\"yolov5s_training_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "\n",
        "print(f\"\\nTraining Configuration:\")\n",
        "print(f\"Model: YOLOv5s\")\n",
        "print(f\"Image size: {img_size}\")\n",
        "print(f\"Batch size: {batch_size}\")\n",
        "print(f\"Epochs: {epochs}\")\n",
        "print(f\"Dataset: {data_path}\")\n",
        "print(f\"Output directory: {project_name}/{experiment_name}\")\n",
        "\n",
        "# Start training\n",
        "print(\"\\nüöÄ Starting training...\")\n",
        "os.chdir('yolov5')\n",
        "\n",
        "training_command = [\n",
        "    'python', 'train.py',\n",
        "    '--img', str(img_size),\n",
        "    '--batch', str(batch_size),\n",
        "    '--epochs', str(epochs),\n",
        "    '--data', data_path,\n",
        "    '--weights', 'yolov5s.pt',\n",
        "    '--project', project_name,\n",
        "    '--name', experiment_name,\n",
        "    '--save-period', '10',\n",
        "    '--device', device\n",
        "]\n",
        "\n",
        "!python train.py --img 640 --batch 16 --epochs 100 --data ../Thesis/dataset/data.yaml --weights yolov5s.pt --project ../Thesis/models --name yolov5s_training --save-period 10 --device 0\n",
        "\n",
        "print(\"\\n‚úÖ Training completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Validate Trained Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import glob\n",
        "import os\n",
        "\n",
        "# Find the trained model weights\n",
        "weights_pattern = \"../Thesis/models/yolov5s_training*/weights/best.pt\"\n",
        "weight_files = glob.glob(weights_pattern)\n",
        "\n",
        "if weight_files:\n",
        "    best_weights = weight_files[0]  # Take the most recent training\n",
        "    print(f\"Found trained weights: {best_weights}\")\n",
        "    \n",
        "    # Run validation\n",
        "    print(\"\\nüîç Running validation...\")\n",
        "    !python val.py --weights {best_weights} --data ../Thesis/dataset/data.yaml --img 640 --save-txt --save-conf --project ../Thesis/metrics --name validation_results\n",
        "    \n",
        "    print(\"\\n‚úÖ Validation completed!\")\n",
        "    print(\"Results saved in: ../Thesis/metrics/validation_results/\")\n",
        "else:\n",
        "    print(\"‚ùå No trained weights found. Please complete training first.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Display Training Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import glob\n",
        "\n",
        "# Find training results directory\n",
        "results_pattern = \"../Thesis/models/yolov5s_training*/\"\n",
        "results_dirs = glob.glob(results_pattern)\n",
        "\n",
        "if results_dirs:\n",
        "    results_dir = results_dirs[0]\n",
        "    print(f\"Loading results from: {results_dir}\")\n",
        "    \n",
        "    # Display training curves if available\n",
        "    results_img = os.path.join(results_dir, \"results.png\")\n",
        "    if os.path.exists(results_img):\n",
        "        print(\"\\nüìä Training Results:\")\n",
        "        img = Image.open(results_img)\n",
        "        plt.figure(figsize=(15, 10))\n",
        "        plt.imshow(img)\n",
        "        plt.axis('off')\n",
        "        plt.title('Training Metrics Over Time')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    \n",
        "    # Display confusion matrix if available\n",
        "    confusion_matrix_img = os.path.join(results_dir, \"confusion_matrix.png\")\n",
        "    if os.path.exists(confusion_matrix_img):\n",
        "        print(\"\\nüéØ Confusion Matrix:\")\n",
        "        img = Image.open(confusion_matrix_img)\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        plt.imshow(img)\n",
        "        plt.axis('off')\n",
        "        plt.title('Confusion Matrix')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    \n",
        "    # Read and display final metrics\n",
        "    results_file = os.path.join(results_dir, \"results.txt\")\n",
        "    if os.path.exists(results_file):\n",
        "        print(\"\\nüìà Final Training Metrics:\")\n",
        "        try:\n",
        "            df = pd.read_csv(results_file, sep='\\s+', header=None)\n",
        "            if len(df) > 0:\n",
        "                last_row = df.iloc[-1]\n",
        "                print(f\"Final Epoch: {int(last_row[0]) + 1}\")\n",
        "                print(f\"mAP@0.5: {last_row[6]:.4f}\")\n",
        "                print(f\"mAP@0.5:0.95: {last_row[7]:.4f}\")\n",
        "                print(f\"Precision: {last_row[4]:.4f}\")\n",
        "                print(f\"Recall: {last_row[5]:.4f}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Could not parse results file: {e}\")\n",
        "            with open(results_file, 'r') as f:\n",
        "                lines = f.readlines()\n",
        "                if lines:\n",
        "                    print(\"Last few lines of results:\")\n",
        "                    for line in lines[-5:]:\n",
        "                        print(line.strip())\n",
        "else:\n",
        "    print(\"‚ùå No training results found.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Export Model to ONNX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "# Find the best trained weights\n",
        "weights_pattern = \"../Thesis/models/yolov5s_training*/weights/best.pt\"\n",
        "weight_files = glob.glob(weights_pattern)\n",
        "\n",
        "if weight_files:\n",
        "    best_weights = weight_files[0]\n",
        "    print(f\"Exporting model from: {best_weights}\")\n",
        "    \n",
        "    # Export to ONNX\n",
        "    print(\"\\nüîÑ Exporting to ONNX format...\")\n",
        "    onnx_output = \"../Thesis/models/yolov5s_best.onnx\"\n",
        "    \n",
        "    export_command = [\n",
        "        'python', 'export.py',\n",
        "        '--weights', best_weights,\n",
        "        '--include', 'onnx',\n",
        "        '--img', '640',\n",
        "        '--batch', '1',\n",
        "        '--device', 'cpu',\n",
        "        '--simplify'\n",
        "    ]\n",
        "    \n",
        "    !python export.py --weights {best_weights} --include onnx --img 640 --batch 1 --device cpu --simplify\n",
        "    \n",
        "    # Move ONNX file to models directory\n",
        "    source_onnx = best_weights.replace('best.pt', 'best.onnx')\n",
        "    if os.path.exists(source_onnx):\n",
        "        import shutil\n",
        "        shutil.copy2(source_onnx, onnx_output)\n",
        "        print(f\"‚úÖ ONNX model saved to: {onnx_output}\")\n",
        "    else:\n",
        "        print(f\"‚ùå ONNX export failed. Expected file: {source_onnx}\")\n",
        "        \n",
        "else:\n",
        "    print(\"‚ùå No trained weights found. Please complete training first.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Convert ONNX to TensorFlow Lite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import onnx\n",
        "import os\n",
        "\n",
        "# Go back to main directory\n",
        "os.chdir('..')\n",
        "\n",
        "onnx_path = \"Thesis/models/yolov5s_best.onnx\"\n",
        "tflite_path = \"Thesis/models/yolov5s_best.tflite\"\n",
        "saved_model_path = \"Thesis/models/yolov5s_saved_model\"\n",
        "\n",
        "if os.path.exists(onnx_path):\n",
        "    print(f\"Converting ONNX model: {onnx_path}\")\n",
        "    \n",
        "    try:\n",
        "        # Install onnx2tf if not already installed\n",
        "        !pip install onnx2tf\n",
        "        \n",
        "        print(\"\\nüîÑ Converting ONNX to TensorFlow SavedModel...\")\n",
        "        # Convert ONNX to TensorFlow SavedModel\n",
        "        !onnx2tf -i {onnx_path} -o {saved_model_path}\n",
        "        \n",
        "        if os.path.exists(saved_model_path):\n",
        "            print(f\"‚úÖ SavedModel created: {saved_model_path}\")\n",
        "            \n",
        "            print(\"\\nüîÑ Converting SavedModel to TensorFlow Lite...\")\n",
        "            # Convert to TensorFlow Lite\n",
        "            converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_path)\n",
        "            converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "            \n",
        "            # Enable quantization for smaller model size\n",
        "            converter.target_spec.supported_types = [tf.float16]\n",
        "            \n",
        "            tflite_model = converter.convert()\n",
        "            \n",
        "            # Save TFLite model\n",
        "            with open(tflite_path, 'wb') as f:\n",
        "                f.write(tflite_model)\n",
        "            \n",
        "            print(f\"‚úÖ TensorFlow Lite model saved: {tflite_path}\")\n",
        "            \n",
        "            # Get file sizes\n",
        "            onnx_size = os.path.getsize(onnx_path) / (1024 * 1024)\n",
        "            tflite_size = os.path.getsize(tflite_path) / (1024 * 1024)\n",
        "            \n",
        "            print(f\"\\nüìä Model Sizes:\")\n",
        "            print(f\"ONNX: {onnx_size:.2f} MB\")\n",
        "            print(f\"TFLite: {tflite_size:.2f} MB\")\n",
        "            print(f\"Compression ratio: {onnx_size/tflite_size:.2f}x\")\n",
        "            \n",
        "        else:\n",
        "            print(\"‚ùå SavedModel conversion failed\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Conversion error: {str(e)}\")\n",
        "        print(\"\\nTrying alternative method with YOLOv5 export...\")\n",
        "        \n",
        "        # Alternative: Export directly from YOLOv5 to TensorFlow\n",
        "        os.chdir('yolov5')\n",
        "        weights_pattern = \"../Thesis/models/yolov5s_training*/weights/best.pt\"\n",
        "        weight_files = glob.glob(weights_pattern)\n",
        "        \n",
        "        if weight_files:\n",
        "            best_weights = weight_files[0]\n",
        "            !python export.py --weights {best_weights} --include saved_model tflite --img 640 --batch 1\n",
        "            \n",
        "            # Move generated files\n",
        "            source_tflite = best_weights.replace('best.pt', 'best-fp16.tflite')\n",
        "            if os.path.exists(source_tflite):\n",
        "                import shutil\n",
        "                shutil.copy2(source_tflite, f\"../{tflite_path}\")\n",
        "                print(f\"‚úÖ TensorFlow Lite model saved: {tflite_path}\")\n",
        "        \n",
        "        os.chdir('..')
        
else:
    print(f"‚ùå ONNX model not found: {onnx_path}")
    print("Please run the ONNX export step first.")\n",
        "        \n",
        "else:\n",
        "    print(f\"‚ùå ONNX model not found: {onnx_path}\")\n",
        "    print(\"Please run the ONNX export step first.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Verify Exported Models and Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"           YOLOv5s TRAINING & EXPORT SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "models_dir = Path(\"Thesis/models\")\n",
        "\n",
        "# Check for PyTorch weights\n",
        "pt_files = list(models_dir.glob(\"**/best.pt\"))\n",
        "if pt_files:\n",
        "    pt_path = pt_files[0]\n",
        "    pt_size = pt_path.stat().st_size / (1024 * 1024)\n",
        "    print(f\"‚úÖ PyTorch Model: {pt_path}\")\n",
        "    print(f\"   Size: {pt_size:.2f} MB\")\n",
        "else:\n",
        "    print(\"‚ùå PyTorch model (.pt) not found\")\n",
        "\n",
        "# Check for ONNX model\n",
        "onnx_path = models_dir / \"yolov5s_best.onnx\"\n",
        "if onnx_path.exists():\n",
        "    onnx_size = onnx_path.stat().st_size / (1024 * 1024)\n",
        "    print(f\"‚úÖ ONNX Model: {onnx_path}\")\n",
        "    print(f\"   Size: {onnx_size:.2f} MB\")\n",
        "else:\n",
        "    # Check for ONNX in training directory\n",
        "    onnx_files = list(models_dir.glob(\"**/*.onnx\"))\n",
        "    if onnx_files:\n",
        "        onnx_path = onnx_files[0]\n",
        "        onnx_size = onnx_path.stat().st_size / (1024 * 1024)\n",
        "        print(f\"‚úÖ ONNX Model: {onnx_path}\")\n",
        "        print(f\"   Size: {onnx_size:.2f} MB\")\n",
        "    else:\n",
        "        print(\"‚ùå ONNX model (.onnx) not found\")\n",
        "\n",
        "# Check for TensorFlow Lite model\n",
        "tflite_path = models_dir / \"yolov5s_best.tflite\"\n",
        "if tflite_path.exists():\n",
        "    tflite_size = tflite_path.stat().st_size / (1024 * 1024)\n",
        "    print(f\"‚úÖ TensorFlow Lite Model: {tflite_path}\")\n",
        "    print(f\"   Size: {tflite_size:.2f} MB\")\n",
        "else:\n",
        "    # Check for TFLite in training directory\n",
        "    tflite_files = list(models_dir.glob(\"**/*.tflite\"))\n",
        "    if tflite_files:\n",
        "        tflite_path = tflite_files[0]\n",
        "        tflite_size = tflite_path.stat().st_size / (1024 * 1024)\n",
        "        print(f\"‚úÖ TensorFlow Lite Model: {tflite_path}\")\n",
        "        print(f\"   Size: {tflite_size:.2f} MB\")\n",
        "    else:\n",
        "        print(\"‚ùå TensorFlow Lite model (.tflite) not found\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)
print("                    FOLDER STRUCTURE")
print("=" * 60)

# Display complete folder structure
def print_tree(directory, prefix="", max_depth=3, current_depth=0):
    if current_depth >= max_depth:
        return
    
    directory = Path(directory)
    if not directory.exists():
        return
        
    items = sorted(directory.iterdir(), key=lambda x: (x.is_file(), x.name.lower()))
    
    for i, item in enumerate(items):
        is_last = i == len(items) - 1
        current_prefix = "‚îî‚îÄ‚îÄ " if is_last else "‚îú‚îÄ‚îÄ "
        print(f"{prefix}{current_prefix}{item.name}")
        
        if item.is_dir() and current_depth < max_depth - 1:
            next_prefix = prefix + ("    " if is_last else "‚îÇ   ")
            print_tree(item, next_prefix, max_depth, current_depth + 1)

print_tree("Thesis")

print("\\n" + "=" * 60)
print("                  ANDROID INTEGRATION")
print("=" * 60)
print("For Android integration, use the .tflite model:")
print("1. Copy the .tflite file to your Android project's assets folder")
print("2. Use TensorFlow Lite Android API to load and run inference")
print("3. Input image size should be 640x640 pixels")
print("4. Model expects normalized input [0,1]")
print("5. Output format: [batch, detections, 85] where 85 = 4(bbox) + 1(conf) + 80(classes)")
print("\\nFor this model: 85 = 4(bbox) + 1(conf) + 2(nsfw, gore)")

print("\\n" + "=" * 60)
print("                    NEXT STEPS")
print("=" * 60)
print("1. Test the exported models with sample images")
print("2. Integrate the .tflite model into your Android application")
print("3. Implement non-maximum suppression (NMS) post-processing")
print("4. Add real-time inference and blurring functionality")
print("5. Optimize performance based on target device specifications")

print("\\nüéâ Training and export pipeline completed successfully!")\n",
        "print(\"